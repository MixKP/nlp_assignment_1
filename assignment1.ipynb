{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12695ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6193a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all text\n",
    "def clean(text):\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # replace new line and tab with space\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "    # remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # collapse multiple spaces into one space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e0e789",
   "metadata": {},
   "source": [
    "# Nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d983ef4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/mix/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/mix/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def Nltk(text):\n",
    "    # Tokenize sentence\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # Clean sentence\n",
    "    cleaned_sentences = [clean(s) for s in sentences]\n",
    "\n",
    "    # Clean text\n",
    "    cleaned_text = clean(text)\n",
    "\n",
    "    # Tokenize words\n",
    "    words = nltk.word_tokenize(cleaned_text)\n",
    "    filtered_words = [w for w in words if w not in stop_words and w.strip() != '']\n",
    "\n",
    "    top_words = Counter(filtered_words).most_common(10)\n",
    "\n",
    "    return cleaned_text, cleaned_sentences, filtered_words, top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0013a7e",
   "metadata": {},
   "source": [
    "# TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81310b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def Textblob(text):\n",
    "    # Create blob from raw text so there still . , for tokenizing sentence\n",
    "    blob_raw = TextBlob(text)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = [str(s) for s in blob_raw.sentences]\n",
    "    # Clean sentences\n",
    "    cleaned_sentences = [clean(s) for s in sentences]\n",
    "    \n",
    "    # Create blob form cleaned text to tokenize word\n",
    "    cleaned_text = clean(text)\n",
    "    # Create blob from cleaned text\n",
    "    blob_cleaned = TextBlob(cleaned_text)\n",
    "\n",
    "    # Filter stop word\n",
    "    filtered_words = [w for w in blob_cleaned.words if w not in stop_words and w.strip() != '']\n",
    "    \n",
    "    # Top Word\n",
    "    top_words = Counter(filtered_words).most_common(10)\n",
    "    \n",
    "    return cleaned_text, cleaned_sentences, filtered_words, top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412c269",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "494a64da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def Spacy(text):\n",
    "    # run spacy on raw text to keep . , for sentences tokenization\n",
    "    spacy_raw = spacy_nlp(text)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = [s.text.strip() for s in spacy_raw.sents]\n",
    "    # Clean sentences\n",
    "    cleaned_sentences = [clean(s) for s in sentences]\n",
    "\n",
    "    cleaned_text = clean(text)\n",
    "    spacy_cleand = spacy_nlp(cleaned_text)\n",
    "    # Tokenize words\n",
    "    filtered_words = [w.text.strip() for w in spacy_cleand if w.text.lower() not in stop_words]\n",
    "\n",
    "    top_words = Counter(filtered_words).most_common(10)\n",
    "\n",
    "    return cleaned_text, cleaned_sentences, filtered_words, top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f9a7bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reports(framework_name, clean_text, sentences, words, top_words, elapsed):\n",
    "    # Create output directory\n",
    "    output_dir = \"output\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Save cleaned text\n",
    "    with open(f\"{output_dir}/cleaned_{framework_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(clean_text)\n",
    "\n",
    "    # Save tokenize sentence and word with their count\n",
    "    with open(f\"{output_dir}/words_{framework_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"----Tokenized Sentences ({len(sentences)})----\\n\")\n",
    "        f.write(\"\\n\".join(f\"{s}\" for s in  sentences))\n",
    "        f.write(f\"----\\n\\nTokenized Words ({len(words)})----\\n\")\n",
    "        f.write(\"\\n\".join(words))\n",
    "\n",
    "    # convert tuple to dataframe and save to textfile\n",
    "    df_top = pd.DataFrame(top_words, columns=[\"Word\", \"Count\"])\n",
    "    with open(f\"{output_dir}/top10words_{framework_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(df_top.to_string(index=False))\n",
    "\n",
    "    time_file = f\"{output_dir}/time_compares.txt\"\n",
    "    \n",
    "    # create data frame for the current run\n",
    "    new_row = {\"Framework\": framework_name, \"Time(s)\": round(elapsed, 6)}\n",
    "    df_new = pd.DataFrame([new_row])\n",
    "    \n",
    "    # check if time compare file already exist or not\n",
    "    if os.path.exists(time_file):\n",
    "        df_existing = pd.read_csv(time_file)\n",
    "        df_final = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "    else:\n",
    "        # if not then it just the row\n",
    "        df_final = df_new\n",
    "    \n",
    "    # save the file with out row number\n",
    "    df_final.to_csv(time_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df920f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework pattern\n",
    "frameworks = [\n",
    "    (Textblob, \"TextBlob\"), (Nltk, \"Nltk\"), (Spacy, \"Spacy\")\n",
    "]\n",
    "\n",
    "# Remove previous time compared.txt to prevent the result of new run to be append to the old file\n",
    "time_file_path = \"output/time_compares.txt\"\n",
    "if os.path.exists(time_file_path):\n",
    "    os.remove(time_file_path)\n",
    "\n",
    "input_file = \"alice29.txt\"\n",
    "with open(input_file, 'r', encoding='utf-8') as file:\n",
    "    raw_text = file.read()\n",
    "\n",
    "# loop through each frameworks \n",
    "for framework_func, name in frameworks:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # run the frameworks\n",
    "    cleaned_text, sentences, final_words_str, top = framework_func(raw_text)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    # record run time\n",
    "    elapsed = end_time - start_time\n",
    "    \n",
    "    # create report for each frameworks\n",
    "    save_reports(name, cleaned_text, sentences, final_words_str, top, elapsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
